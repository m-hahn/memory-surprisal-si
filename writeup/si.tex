\documentclass[11pt,letterpaper]{article}

\usepackage{showlabels}
\usepackage{fullpage}
\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}



\usepackage{caption}

\usepackage{tikz-dependency}
\usepackage{longtable}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\Ff}[0]{\mathcal{F}}

\usepackage{multirow}

\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}
\newcommand\note[1]{{\color{red}(#1)}}
\newcommand\jd[1]{{\color{red}(#1)}}
\newcommand\rljf[1]{{\color{red}(#1)}}
\newcommand{\key}[1]{\textbf{#1}}

\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}


\frenchspacing
%\def\baselinestretch{0.975}

%\emnlpfinalcopy
%\def\emnlppaperid{496}

\title{Supplementary Information for: Crosslinguistic Word Orders Enable an Efficient Tradeoff between Memory and Surprisal}
\author{Michael Hahn, Judith Degen, Richard Futrell}
\date{2018}

\begin{document}

\maketitle




%
%\begin{center}
%\includegraphics[width=0.5\textwidth]{../code/analysis/visualize_neural/figures/full-REAL-listener-surprisal-memory-HIST_z_byMem_onlyWordForms_boundedVocab.pdf}
%\captionof{figure}{Histogram}\label{fig:hist-real}
%\end{center}
%
%


\section{Formal Analysis and Proofs}

In this section, we prove Theorem 1.

\subsection{Mathematical Assumptions}

We first make explicit how we formalize language processing for proving the theorem.


\paragraph{Ingredient 1: Language as a Stationary Stochastic Process}
We represent language as a stochastic process of words $\dots w_{-2} w_{-1} w_0 w_{1} w_{2} \dots$, extending indefinitely both into the past and into the future.
The symbols $w_i$ belong to a common set, representing the words of the language.\footnote{Could also be phonemes, sentences, or any other kind of unit.}

%We model the sequence as a probabilistic sequence; that is, given a context $w_{<t}$, the next word is distributed according to a distribution $p(w_t|w_{<t})$.

The assumption of infinite length is for mathematical convenience and does not affect the substance of our results:
As we restrict our attention to the processing of individual sentences, which have finite length, we will actually not make use of long-range and infinite contexts.

We make the assumption that this process is \emph{stationary}.
Formally, this means that the conditional distribution $P(w_t|w_{<t})$ does not depend on $t$, it only depends on the (semi-infinite) context sequence $w_{<t}$.
Informally, this says that the process has no `internal clock', and that the statistical rules of the language do not change at the timescale we are interested in.
In reality, the statistical rules of language do change: They change as language changes over generations, and they also change between different situations -- e.g., depending on the interlocutor at a given point in time.
Given that we are interested in memory needs in the processing of \emph{individual sentences}, at a timescale of seconds or minutes, stationarity seems to be a reasonable assumption to make.


%
%
%
%\begin{figure}
%\includegraphics[width=0.45\textwidth]{figures/markov-condition.png}
%	\caption{Illustration of (\ref{eq:listener-markov}). As the utterance unfolds, the listener maintains a memory state. After receiving word $w_t$, the listener computes their new memory state $m_t$ based on the previous memory state $m_{t-1}$ and the new word $w_t$.}\label{fig:listener-markov}
%\end{figure}
%

\paragraph{Ingredient 2: Postulates about Processing}
The second ingredient consists of the three postulates described in the main paper.
%We now analyze memory from the perspective of the listener, who needs to maintain information about the past to predict the future.
%As the speaker's utterance unfolds, the listener maintains a memory state $m_t$.
There are no further assumptions about the memory architecture and the nature of its computations.
%We only make a basic assumption about the flow of information (Figure~\ref{fig:listener-markov}):
%At a given point in time, the listener's memory state $m_t$ is determined by the last word $w_t$, and the prior memory state $m_{t-1}$:
%\begin{equation}
%	m_t = M(m_{t-1}, w_t)
%\end{equation}
%As a consequence, $m_t$ contains no information about the process beyond what is contained in the last word observed $w_{t-1}$ and in the memory state before that word was observed $m_{t-1}$.
%%This is formalized as a statement about conditional probabilities:
%%\begin{equation}\label{eq:listener-markov}
%%p(m_1| (w_{t})_{t \in \mathbb{Z}}, m_0)   = p(m_1 | m_0, w_1)
%%\end{equation}
%%This says that $m_1$ contains no information about the utterances beyond what is contained in $m_0$ and $w_1$.	
%As a consequence, the listener has no knowledge of the speaker's state beyond the information provided in their prior communication.
%This is a simplification, as the listener could obtain information about the speaker from other sources, such as their common environment (weather, ...).
%\mhahn{For the study of memory in sentence processing, this seems fair. Discuss this more.}

%
%First, we assume that the listener's internal state cannot depend on the future beyond its dependency on the past.
%Formally: 
%\begin{equation}\label{eq:listener-markov-1}
%m_t \bot w_{>t} | w_{\leq t}
%\end{equation}
%This means that the listener has no access to the speaker's state beyond what the speaker has already uttered.
%
%Second, we assume that $m_t$ contains no information about the past beyond what is contained in $w_{t-1}$ and $m_{t-1}$:
%\begin{equation}\label{eq:listener-markov-2}
%m_t \bot w_{<t} | w_{t-1}, m_{t-1}
%\end{equation}
%This means that any information about the past in $m_t$ has to be contained in $m_{t-1}$ -- formalizing the idea that a listener can only remember aspects of the past by keeping them in memory, and that memories of the past cannot `spontaneously' form later in the future.

%The listener can trade off memory and future surprisal:
%A listener who chooses to store less memory will exerience higher surprisal in the future.
%A listener can achieve minimal surprisal -- that is, the lowest average surprisal that any model could achieve by predicting the future from the past -- if and only if $m_t$ contains all predictive information about the future that is contained in the past.

%We now describe the memory-surprisal tradeoff. will describe this tradeoff, and show that listener memory is linked to locality in a way similar to speaker memory.
%Consider a listener who uses $J$ bits of memory on average.
%What can we say about the listener's surprisal?

\subsection{Proof of the Theorem}

We restate the theorem:

\begin{thm}\label{prop:suboptimal}
	Let $T$ be any positive integer ($T \in \{1, 2, 3, ...\}$), and consider a listener using at most
	\begin{equation}\label{eq:memory}
		\sum_{t=1}^T t I_t
	\end{equation}
bits of memory on average.
Then this listener will incur surprisal at least
	$$H[w_t|w_{<t}] + \sum_{t > T} I_t$$
	on average.
\end{thm}



%We formalize a language as a stationary stochastic process $\dots w_{-2} w_{-1} w_0 w_{1} w_{2} \dots$, extending indefinitely both into the past and into the future.
%The symbols $w_i$ belong to a common set, representing the words of the language.\footnote{Could also be phonemes, sentences, ..., any other kind of unit.}
%We denote the listener's memory state at time $t$, after hearing $w_{<t} = ... w_{t-2} w_{t-1}$ by $m_t$.
%As described above, we assume
%\begin{equation}
%	m_t = M(m_{t-1}, w_{t-1})
%\end{equation}
%\footnote{Alternatively we could admit nondeterministic memory encodings, and require
%\begin{equation}\label{eq:listener-markov}
%	p(m_{t+1}| (w_{t'})_{t' \in \mathbb{Z}}, m_t)   = p(m_{t+1} | m_t, w_{t})
%\end{equation}
%that is, $m_{t+1}$ contains no information about the utterances beyond what is contained in $m_t$ and $w_{t}$.}
%As a consequence, the listener has no knowledge of the speaker's state beyond the information provided in their prior communication.


%The average number of bits required to encode this state is $\operatorname{H}[m_t]$, which by assumption is at most $\sum_{t=1}^T t I_t$.
%As the listener's predictions are made on the basis of her memory state, her average surprisal is at least $\operatorname{H}[w_t | m_t]$.
\begin{proof}
The difference between the listener's surprisal and optimal surprisal is $\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]$.\footnote{A listener whose predictions are not optimal given $m_t$ can only incur even higher surprisal.}
By the assumption of stationarity, we can, for any positive integer $T$, rewrite this expression as
\begin{equation}\label{eq:byStation}
\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}] =  \frac{1}{T} \sum_{t'=1}^{T} \left(\operatorname{H}[w_{t'} | m_{t'}] - \operatorname{H}[w_{t'} | w_{<t'}]\right) 
\end{equation}
%NOTE the proof could be made simpler by taking $m_t$ to be a deterministic function of $x_t$, $m_{t-1}$, rather than talking about conditional independence
%
%We first show a lemma:
%
%\begin{lemma}
%For any positive integer $t$, the following inequality holds:
%\begin{equation}
%H[w_t | m_t] \geq H[w_t|w_{1 \dots t-1}, m_1]
%\end{equation}
%\end{lemma}
%
%\begin{proof}[Proof of the Lemma]
Because $m_t$ is determined by $(w_{1 \dots t-1}, m_1)$:
\begin{equation}
	m_t = M(m_{t-1}, w_{t-1}) = M(M(m_{t-2}, w_{t-2}), w_{t-1}) = M(M(M(m_{t-3}, w_{t-3}), w_{t-2}), w_{t-1}) = \dots
\end{equation}
the Data Processing inequality entails the following inequality for every positive integer $t$:
\begin{align}\label{eq:plugged}
\operatorname{H}[w_t | m_t]& \geq \operatorname{H}[w_t|w_{1\dots t-1}, m_1]
\end{align}
Plugging this inequality into Equation~\ref{eq:byStation} above:
\begin{align}\label{eq:plugged}
\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]& \geq \frac{1}{T} \sum_{t=1}^T ( \operatorname{H}[w_t|w_{1\dots t-1}, m_1] - \operatorname{H}[w_t | w_{1\dots t-1}, w_{\leq 0}]  )    \\
& = \frac{1}{T} \left(\operatorname{H}[w_{1\dots T} | m_1] - \operatorname{H}[w_{1\dots T} | w_{\leq 0}]\right)  \\
& = \frac{1}{T} \left(I[w_{1\dots T}, w_{\leq 0}] - I[w_{1\dots T}, m_1]\right) 
\end{align}
The first term $I[w_{1\dots T}, w_{\leq 0}]$ can be rewritten in terms of $I_t$:
\begin{align}\label{eq:i-expanded}
I[w_{1\dots T}, w_{\leq 0}] &= \sum_{i=1}^T \sum_{j=-1}^{-\infty} I[w_i, w_j | w_{j+1}...w_{i-1}] = \sum_{t=1}^T t I_t + T \sum_{t > T} I_t
\end{align}
Therefore
\begin{align*}
\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]& \geq \frac{1}{T} \left(\sum_{t=1}^T t I_t + T \sum_{t > T} I_t - I[w_{1\dots T}, m_1]\right) 
\end{align*}
$I[w_{1\dots T}|m_1]$ is at most $\operatorname{H}[m_1]$, which is at most $\sum_{t=1}^T t I_t$ by assumption. Thus, the expression above is bounded by
\begin{align*}
\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]& \geq \frac{1}{T} \left(\sum_{t=1}^T t I_t + T \sum_{t > T} I_t - \sum_{t=1}^T t I_t\right) \\
&= \sum_{t > T} I_t
\end{align*}
Rearranging shows that the listener's surprisal is at least $\operatorname{H}[w_t|m_t] \geq \operatorname{H}[w_t | w_{<t}] + \sum_{t > T} I_t$, as claimed.
\end{proof}


%Justifylinear interpolation: The curve is convex, which is shown by `time-sharing': Use one code $\lambda$ fraction of times, and the other code $1-\lambda$ fraction of times.



\subsection{For nondeterministic encoding functions}

We have been assuming that $m_t$ is a deterministic function of $x_t$ and $m_{t-1}$.
Here, we show that this assumption can be relaxed to stochastic encoding functions.

We relax condition (X) to the following requirement, for all values of $m_1, (w_{t})_{t \in \mathbb{Z}}, m_0$:
\begin{equation}\label{eq:listener-markov-nondeterministic}
p(m_1| (w_{t})_{t \in \mathbb{Z}}, m_0)   = p(m_1 | m_0, w_1)
\end{equation}
This says that $m_1$ contains no information about the utterances beyond what is contained in $m_0$ and $w_1$.	

The one place in the proof where (X) plays a role is the proof of the inequality:
\begin{equation}
	H[w_t | m_t] \geq H[w_t|w_{1 \dots t-1}, m_1]
\end{equation}

We show that this inequality still holds under the relaxed condition (\ref{eq:listener-markov-nondeterministic}):
\begin{proof}
	By Bayes' Theorem
\begin{align*}
	p(w_t|m_0, m_1, w_{0\dots t-1}) &= \frac{p(m_1|m_0, w_{0\dots t})}{p(m_1|m_0, w_{0\dots t-1})} \cdot p(w_t|m_0, w_{0\dots t-1}) \\
 &= \frac{p(m_1|m_0, w_{0})}{p(m_1|m_0, w_{0})} \cdot p(w_t|m_0, w_{0\dots t-1}) \\
 &= p(w_t|m_0, w_{0\dots t-1}) \\
\end{align*}
	where the second equation follows from (\ref{eq:listener-markov-nondeterministic}).
So we have a Markov chain
\begin{equation}
(w_t) \rightarrow (m_0, w_{0 \dots t-1})   \rightarrow   (m_1, w_{1 \dots t-1})
\end{equation}
Thus, by the Data Processing Inequality,
\begin{equation}
H[w_t| w_{1 \dots t-1}, m_{1}] \geq H[w_t|w_{0 \dots t-1}, m_0]
\end{equation}
Finally, iteratively applying this reasoning, we conclude:
\begin{align*}
H[w_t | m_t] \geq H[w_t| w_{t-1}, m_{t-1}] \geq H[w_t| w_{t-2, t-1}, m_{t-2}] \geq ... \geq H[w_t|w_{1 \dots t-1}, m_1]
\end{align*}
\end{proof}


\subsection{Locality in a model with Memory Retrieval}

Here we show that our information-theoretic analysis is compatible with models placing the main bottleneck in the difficulty of retrieval \citep{mcelree2000sentence,lewis-activation-based-2005,nicenboim2018models,vasishth2019computational}.
We extend our model of memory in incremental prediction to capture key aspects of the models described by \citet{lewis-activation-based-2005,nicenboim2018models,vasishth2019computational}.

The ACT-R model of \cite{lewis-activation-based-2005} assumes a small working memory consisting of \emph{buffers} and a \emph{control state}, which together hold a small and fixed number of individual \emph{chunks}.
It also assumes a large short-term memory that contains an unbounded number of chunks.
This large memory store is accessed via \emph{cue-based retrieval}: a query is constructed based on the current state of the buffers and the control state; a chunk that matches this query is then selected from the memory storage and placed into one of the buffers.

\paragraph{Formal Model}
We extend our information-theoretic analysis by considering a model that maintains both a small working memory $m_t$ -- corresponding to the buffers and the control state -- and an unlimited short-term memory $s_t$.
Predictions are made based on working memory $m_t$, incurring surprisal $H[w_t|m_t]$.
When processing a word $x_t$, there is some amount of communication between $m_t$ and $s_t$, corresponding to retrieval operations.
We model this using a variable $r_t$ representing the information that is retrieved from $s_t$.
In our formalization, $r_t$ reflects the totality of all retrieval operations that are made during the processing of $x_{t-1}$; they happen after $x_{t-1}$ has been observed but before $x_t$ has.

The working memory state is determined not just by the input $x_t$ and the previous working memory state $m_{t-1}$, but also by the retrieved information:
\begin{equation}
	m_t = f(x_t, m_{t-1}, r_t) 
\end{equation}
The retrieval operation is jointly determined by working memory, short-term memory, and the previous word:
\begin{equation}\label{eq:rt}
	r_t = g(x_{t-1}, m_{t-1}, s_{t-1}) 
\end{equation}
Finally, the short-term memory can incorporate any -- possibly all -- information from the last word and the working memory:
\begin{equation}
	s_t = h(x_{t-1}, m_{t-1}, s_{t-1}) 
\end{equation}
While $s_t$ is unconstrained, there are constraints on the capacity of working memory $\operatorname{H}[m_t]$ and the amount of retrieved information $\operatorname{H}[r_t]$.
Placing a bound on $\operatorname{H}[m_t]$ reflects the fact that the buffers can only hold a small and fixed number of chunks \citep{lewis-activation-based-2005}.


\paragraph{Cost of Retrieval}
%In ACT-R, each retrieval operation is initiated through a query constructed based on the current state of the buffers; it returns a chunk from short-term memory that is then placed into a buffer.
%Equation~\ref{eq:rt} reflects that these retrieved chunks are determined by working memory and short-term memory.
In the model of \cite{lewis-activation-based-2005}, the time it takes to process a word is determined primarily by the time spent retrieving chunks, which is determined by the number of retrieval operations and the time it takes to complete each retrieval operation.
If the information content of each chunk is bounded, then a bound on $H[r_t]$ corresponds to a bound on the number of retrieval operations.

In the model of \cite{lewis-activation-based-2005}, a retrieval operation takes longer if more chunks are similar to the retrieval cue, whereas, in the direct-access model \citep{mcelree2000sentence,nicenboim2018models,vasishth2019computational}, retrieval operations take a constant amount of time.
There is no direct counterpart to differences in retrieval times and similarity-based inhibition as in the activation-based model in our formalization.
Our formalization thus more closely matches the direct-access model, though it might be possible to incorporate aspects of the activation-based model in our formalization.

\paragraph{Role of Surprisal}
The ACT-R model of \cite{lewis-activation-based-2005} does not have an explicit surprisal cost.
Instead, surprisal effects are interpreted as arising because, in less constraining contexts, the parser is more likely to make decisions that then turn out to be incorrect, leading to additional correcting steps.
%We see this as an algorithmic-level implementation of the justification for surprisal theory provided by \citet{levy2008expectation}:
We view this as an algorithmic-level implementation of a surprisal cost $H[x_t|m_{t-1}]$:
If the word $x_t$ is unexpected given the current state of the working memory -- i.e., buffers and control states -- then their current state must provide insufficient information to constrain the actual syntactic state of the sentence, meaning that the parsing steps made to integrate $x_t$ are likely to include more backtracking and correction steps.
Thus, we argue that cue-based retrieval models predict that the surprisal $- \log P(x_t|m_{t-1})$ will be part of the cost of processing word $x_t$.


%This is instantiated by ACT-R.
%We can explicitly explain how this covers the McElree ideas and the Lewis and Vasishth ACT-R model.
%This model has two bottlenecks:
%The working memory capacity, which we model as $H[m_t]$, and the amount of information that is added through retrieval, modeled as $H[r_t|m_t]$.
%Bounding retrieval = bounding the precision and/or content of retrieved items. Explain more how this relates to McElree and ACT-R.
%In ACT-R, each retrieval operation takes time. If each chunk has a bounded amount of information, then this corresponds to a bottleneck in $H[r_t|m_t]$

\paragraph{Theoretical Result}
We now show an extension of our theoretical result in the setting of the retrieval-based model described above.

\begin{thm}
Let $0 < S \leq T$ be positive integers such that the average working memory cost $\operatorname{H}[m_t]$ is bounded as
	\begin{equation}
		\operatorname{H}[m_t] \leq \sum_{t=1}^T t I_t
	\end{equation}
	and the average amount of retrieved information is bounded as
	\begin{equation}
		\operatorname{H}[r_t] \leq \sum_{t=T+1}^S I_t
	\end{equation} %(per word).
	Then the surprisal cost is lower-bounded as
	\begin{equation}
		\operatorname{H}[w_t|m_t] \geq \operatorname{H}[w_t|x_{<t}] + \sum_{t>S} I_t
	\end{equation}
\end{thm}

\begin{proof}
The proof is a generalization of the proof above.
	For any positive integer $t$, $m_t$ is determined by $w_{1\dots t}, m_0, r_0, \dots, r_t$.
	Therefore, the Data Processing Inequality entails:
	\begin{equation}
		\operatorname{H}[w_t|m_t] \geq \operatorname{H}[w_t|w_{1\dots t}, m_0, r_0, \dots, r_t]
	\end{equation}
	As in~(\ref{eq:plugged}), this leads to
\begin{align}
\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]& \geq \frac{1}{T} \sum_{t=1}^T ( \operatorname{H}[w_t|w_{1\dots t}, m_0, r_0, \dots, r_t] - \operatorname{H}[w_t | w_{1\dots t-1}, w_{\leq 0}]  )    \\
& \geq \frac{1}{T} \left(\operatorname{H}[w_{1\dots T} | m_0, r_0, \dots, r_T] - \operatorname{H}[w_{1\dots T} | w_{\leq 0}]\right)  \\
	& = \frac{1}{T} \left(I[w_{1\dots T}, w_{\leq 0}] - I[w_{1\dots T}, (m_0, r_0, \dots, r_T)]\right) 
\end{align}
	Now, using the calculation from (\ref{eq:i-expanded}), this can be rewritten as:
	\begin{align*}
\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]= & \frac{1}{T}\left(\sum_{t=1}^T t I_t + T \sum_{t>T} I_t - I[X_1\dots X_T, (M_0, R_1, ..., R_T)]\right) \\
		= & \frac{1}{T}\left(\sum_{t=1}^T t I_t + T \sum_{t>T} I_t - I[X_{1\dots T}, M_0] - \sum_{t=1}^T I[X_{1\dots T}, R_t|M_0, r_{1\dots t-1}]\right) \\
	\end{align*}
	Due to the inequalities $I[X_{1\dots T}, M_0] \leq \operatorname{H}[M_0]$ and $\operatorname{I}[X_{1\dots T}, R_t|M_0, r_{1\dots t-1}] \leq \operatorname{H}[R_t]$, this can be bounded as
	\begin{align}
\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]		\geq & \frac{1}{T}\left(\sum_{t=1}^T t I_t  + T \sum_{t>T} I_t-H[M_0] - \sum_{t=1}^T H[R_t]\right) \\
	\end{align}
	Finally, this reduces as
	\begin{align}
	\operatorname{H}[w_t | m_t] - \operatorname{H}[w_t | w_{<t}]		\geq &  \frac{1}{T}(T \sum_{t>T} I_t - T\cdot H[R_t]) \\
	= & \sum_{t>T} I_t- H[R_t]  \\
		\geq & \sum_{t>T} I_t - \sum_{t=T+1}^S I_t \\
		= &  \sum_{t>S} I_t
\end{align}

\end{proof}

\paragraph{Information Locality}
We now show that this result predicts information locality provided that retrieving information is more expensive than keeping the same amount of information in working memory.
For this, we formalize the problem of finding an optimal memory strategy as a multi-objective optimization, aiming to minimize
\begin{equation}
	\lambda_1 H[m_t] + \lambda_2 H[r_t]
\end{equation}
to achieve a given surprisal level, for some setting of $\lambda_1, \lambda_2 > 0$ describing the relative cost of storage and retrieval.
What is the optimal division of labor between keeping information in working memory and recovering it through retrieval?
The problem
\begin{equation}\label{eq:multi-obj-t}
	\min_{T} \lambda_1 \sum_{t=1}^T t I_t + \lambda_2 \sum_{t=T+1}^S I_t
\end{equation}
has solution $T \approx \frac{\lambda_2}{\lambda_1}$. %\footnote{Can do simple proof using the continuous-$T$-version.}
This means that, as long as retrievals are more expensive than keeping the same amount of information in working memory (i.e., $\lambda_2 > \lambda_1$), the optimal strategy stores information from the last $T > 1$ words in working memory.
Due to the factor $t$ inside $\sum_{t=1}^T t I_t$, the bound~(\ref{eq:multi-obj-t}) will be reduced when $I_t$ decays faster, i.e., there is strong information locality.

The assumption that retrieving information is more difficult than storing it is reasonable for cue-based retrieval models, as retrieval suffers from similarity-based interference effects due to the unstructured nature of the storage~\citep{lewis-activation-based-2005}.
A model that maintains no information in its working memory, i.e. $H[m_t]=0$, would correspond to a cue-based retrieval model that stores nothing in its buffers and control states, and relies entirely on retrieval to access past information.
Given the nature of representations assumed in models~\citep{lewis-activation-based-2005}, such a model would seem to be severely restricted in its ability to parse language.

%If $\frac{\lambda_2}{\lambda_1} \rightarrow \infty$ (retrievals get more expensive), recover previous model.
%If $\frac{\lambda_2}{\lambda_1} \rightarrow 0$ (retrievals get cheaper), locality effect gets weaker, and disappears in the limit\footnote{(Of course, even in this limit, there might be additional factors that may still favor locality in a specific implementation of memory -- e.g., in ACT-R, decay and interference are less problematic if there is locality.)}




\subsection{Results for Language Production}

Here we show results linking memory and locality in production.
We show that results similar to our main theorem hold for the tradeoff between a speaker's memory and the accuracy with which they match the distribution of the language.



\paragraph{Speaker aims to match language distribution}
First, we consider a setting in which a speaker produces sentences with bounded memory, and analyze the deviation of the produced distribution from the actual distribution of the language.

We consider a speaker who maintains memory representations and incrementally produces based on these representations:
\begin{equation}
	p_{speaker}(x_t|X_{<t}) = p(x_t|m_t) \\
\end{equation}
We show a tradeoff between the memory capacity $\operatorname{H}[m_t]$ and the KL-divergence between the actual language statistics and the speaker's production distribution:
\begin{equation}
D_{KL}(P_{language}||P_{produced})  := \E_{X_{<t}} \sum_{x_t} p(x_t|X_{<t}) \log \frac{p(x_t|X_{<t})}{p_{speaker}(x_t|X_{<t})} \\
\end{equation}
\begin{thm}
If a speaker maintains memory
	\begin{equation}
		\operatorname{H}[m_t] \leq \sum_{i=1}^T tI_t
	\end{equation}
	then 
\begin{equation}
	D_{KL}(P_{language}||P_{produced}) \geq \sum_{t=T+1}^\infty I_t
\end{equation}
\end{thm}

While this bound only considers the production of a single word, it immediately entails a bound on the production accuracy for sequences:
\begin{equation}
	D_{KL}(P_{language}(X_1\dots X_t|X_{\leq 0})||P_{produced}(X_1\dots X_t|X_{\leq 0}))  = t \cdot D_{KL}(P_{language}(X_1|X_{\leq 0})||P_{produced}(X_1|X_{\leq 0}))
\end{equation}



\begin{proof}
First note
\begin{align}
D_{KL}(P_{language}||P_{produced}) & = \E_{X_{<t}} \sum_{x_t} p(x_t|X_{<t}) \log \frac{p(x_t|X_{<t})}{p_{speaker}(x_t|X_{<t})} \\
& = \E_{X_{<t}} \sum_{x_t} p(x_t|X_{<t}) \log \frac{p(x_t|X_{<t})}{p(x_t|M(X_{<t}))} \\
& = \E_{X_{<t}} \sum_{x_t} p(x_t|X_{<t}) \log p(x_t|X_{<t}) - \E_{X_{<t}} \sum_{x_t} p(x_t|X_{<t}) \log p(x_t|M(X_{<t})) \\
	& = \E_{X_{<t}} \sum_{x_t} p(x_t|X_{<t}) \log p(x_t|X_{<t}) + S_M(x_t|X_{<t})
\end{align}
In the last line, the first term is a constant independent of $M$.

Then the proof for the listener case transfers to this setting.
\end{proof}

TODO limitations of this as a model of production




\paragraph{Speaker aims to match, conditional on goal}
The first setting does not account for the fact that language is produced aiming for some communicative goal.
We therefore now assume that the speaker has a communicative goal $G$ in mind.
This goal $G$ stays constant during production process for a sentence, and we count how much memory is needed in addition to the goal $G$.
We assume that there is a distribution of sentences expressing goals $G$:
\begin{equation}
	P(sentence|G)
\end{equation}
and assume that the speaker aims to match this distribution
\begin{equation}
\mathbb{E}_G[D_{KL}((language|G)||(produced|G))]
\end{equation}
We can analyze this model by adding conditioning w.r.t. $G$ throughout the analysis of the previous case.
Specifically, we need
\begin{equation}
I_t^G := I[X_t, X_0|X_1, \dots, X_{t-1}, G]
\end{equation}

Take $I_t$ conditioned on $G$: only count statistical dependencies to the degree that they are not redundant with the goal


\begin{thm}
If a speaker maintains memory
	\begin{equation}
		\operatorname{H}[m_t] \leq \sum_{i=1}^T tI_t^G
	\end{equation}
	then 
\begin{equation}
\E_G	D_{KL}(P_{language}(\cdot|G)||P_{produced}(\cdot|G)) \geq \sum_{t=T+1}^\infty I_t^G
\end{equation}
\end{thm}

\begin{proof}
This is entirely analogous to the previous proof.
\end{proof}


are there conditions under which this is close to $I_t$?

\paragraph{Pragmatic Speaker}

would need an assumption on the density of goals in the space of sequences.


Note 
\begin{equation}
	D_{KL}(P_{language}(x_{1\dots t})||P_{produced}(x_{1\dots t}))  := \sum_{x_{1\dots t}} p(x_t|X_{1\dots t}) \log \frac{p(x_t|X_{1\dots t})}{p_{speaker}(x_t|X_{1\dots t})} \geq t D_KL(x_t||...)
\end{equation}

$H[G|Produced] - H[G|Language]$



\section{Proof of Left-Right Invariance}

Here we show that the bound provided by our theorem is invariant under reversal of the process.
That is: Given a process $(X_t)_{t \in \mathbb{Z}}$, we define its reverse process $(Y_t)_{t \in \mathbb{Z}}$ by $Y_t := X_t$.
We claim that the theorem provides the same bounds for the memory-surprisal tradeoff curves.
To prove this, we note:
\begin{equation}
	I[X_t, X_0|X_{1\dots t-1}] = I[Y_{-t}, Y_0|Y_{1-t\dots -1}] = I[Y_0, Y_t|Y_{1\dots t-1}] = I[Y_t, Y_0|Y_{1\dots t-1}]
\end{equation}
The first step follows from the definition of $Y$. The second step follows from the fact that $X_t$, and thus also $Y_t$, is stationary, and thus adding $t$ to each index in the expression does not change the resulting value. The third step uses the fact that mutual information is symmetric.


\section{Example where window model is not optimal}

\subsection{Example I}

Here we provide an example of a stochastic process where a window-based memory encoding is not optimal, but the bound provided by our theorem still holds.

Let $k$ be some positive integer.
Consider a process
$x_{t+1} = (v_{t+1}, w_{t+1}, y_{t+1}, z_{t+1})$
where
\begin{enumerate}
	\item The first two components consist of fresh random bits. Formally, $v_{t+1}$ is an independent draw from $Bernoulli(0.5)$, independent from all preceding observations $x_{\leq t}$.
		Second, let $w_{t+1}$ consist of $2k$ many such independent random bits (so that $H[w_{t+1}] = 2k$)
	\item The third component \emph{deterministically} copies the first bit from $2k$ steps earlier. Formally, $y_{t+1}$ is equal to the first component of $x_{t-2k+1}$
	\item The fourth component \emph{stochastically} copies the second part (consisting of $2k$ random bits) from one step earlier. Formally, each component $z_{t+1}^{(i)}$ is determined as follows: First take a sample $u_{t+1}^{(i)}$ from $Bernoulli(\frac{1}{4k})$, independent from all preceding observations.
		If $u_{z+1}^{(i)}=1$, set $z_{t+1}^{(i)}$ to be equal to the second component of $w_{t}^{(i)}$.
		Otherwise, let $z_{t+1}^{(i)}$ be a fresh draw from $Bernoulli(0.5)$.
\end{enumerate}

Predicting observations optimally requires taking into account observations from the $2k$ last time steps.

We show that, when approximately predicting with low memory capacities, a window-based approach does \emph{not} in general achieve an optimal memory-surprisal tradeoff.

Consider a model that predicts $x_{t+1}$ from only the last observation $x_t$, i.e., uses a window of length one.
The only relevant piece of information in this past observation is $w_t$, which stochastically influences $z_{t+1}$.
Storing this costs $2k$ bit of memory as $w_t$ consists of $2k$ draws from $Bernoulli(0.5)$.
How much does it reduce the surprisal of $x_{t+1}$?
Due to the stochastic nature of $z_{t+1}$, it reduces the surprisal only by about $I[x_{t+1}, w_t] = I[z_{t+1}, w_t] < 2k \cdot \frac{1}{2k} = 1$, i.e., surprisal reduction is strictly less than one bit.
\footnote{We can evaluate $I[z_{t+1}, w_t]$ as follows. Set $l = k/4$. Write $z, w$ for any of the $2k$ components of $z_{t+1}, w_t$, respectively. First, calculate $p(z = 1|w=1) = 1/l + (1-1/l) \frac{1}{2} = 1/(2l) + 1/2 = \frac{1+l}{2l}$ and $p(z = 0|w=1) = (1-1/l) \frac{1}{2} = 1/2 - 1/2l = \frac{l-1}{2l}$.
Then $I[Z, W] = D_{KL}(p(z|w=1)||p(z)) = \frac{1+l}{2l} \log \frac{\frac{1+l}{2l}}{1/2} + \frac{l-1}{2l} \log \frac{\frac{l-1}{2l}}{1/2}  = \frac{1+l}{2l} \log \frac{1+l}{l} + \frac{l-1}{2l} \log \frac{l-1}{l}  \leq \frac{1+l}{l} \log \frac{1+l}{l} =  (1+1/l) \log (1+1/l)  \leq  (1+1/l) (1/l) = 1/l + 1/l^2 < 2/l = \frac{1}{2k}.$
%Then $H[Z|w=1] = - \frac{1+k}{2k} \log \frac{1+k}{2k} - \frac{k-1}{2k} \log \frac{k-1}{2k}  = - (1/2 + 1/2k) \log \frac{1+k}{2k} - (1/2 - 1/2k) \log \frac{k-1}{2k}$ %= - \frac{1+k}{2k} \log (1+k) - \frac{k-1}{2k} \log (k-1) + \log 2k = - \frac{1+k}{2k} \log (1+k) - \frac{k-1}{2k} \log (k-1) + \log 2k$.
}

We show that there is an alternative model that strictly improves on this window-based model:
Consider a memory encoding model that encodes each of $v_{t-2k+1}, \dots, v_{t}$, which costs $2k$ bits of memory -- as the window-based model did.
Since $y_{t+1} = v_{t-2k+1}$, this model achieves a surprisal reduction of $H[v_{t-2k+1}] = 1$ bit, strictly more than the window-based model.


This result does not contradict our theorem because the theorem only provides \emph{bounds} across models, which are not necessarily achieved by a given window-based model.
In fact, for the process described here, no memory encoding function $M$ can exactly achieve the theoretical bound described by the theorem.

\subsection{Example II}

\textbf{TODO} we could also provide an example where our bound is relatively tight for the retrieval-based model even though it is very loose for the capacity model.

\section{Corpus Size per Language}

\begin{center}
\begin{longtable}{l|ll||l|llllllllllllll}
	Language & Training & Held-Out & 	Language & Training & Held-Out\\ \hline
\input{tables/corpusSizes.tex}
\end{longtable}
	\captionof{table}{Languages, with the number of training and held-out sentences available.}\label{tab:corpora}
\end{center}

\section{Samples Drawn per Language}

\begin{center}
\begin{longtable}{l|ll||l|llllllllllllll}
	Language & Base. & Real & Language & Base. & Real \\ \hline
\input{tables/samplesNumber.tex}
\end{longtable}
	\captionof{figure}{Samples drawn per language according to the precision-dependent stopping criterion.}\label{tab:samples}
\end{center}



\begin{center}
\begin{longtable}{l|lll||l|lllllllllllllll}
	Language & Mean & Lower & Upper & Language & Mean & Lower & Upper \\ \hline
\input{tables/boot_g_REAL.tex}
\end{longtable}
	\captionof{figure}{Bootstrapped estimates for $G$.}\label{tab:boot-g}
\end{center}


\section{Detailed Results per Language}

%\subsection{Median Surprisal per Memory Budget}
%
%\begin{center}
%\begin{longtable}{ccccccccccccccclll}
%\input{tables/medians_REAL_0.tex}
%\end{longtable}
%	\captionof{figure}{Medians: For each memory budget, we provide the median surprisal for real and random languages. Solid lines indicate sample medians, dashed lines indicate 95 $\%$ confidence intervals for the population median, dotted lines indicate empirical quantiles ($10\%, 20\%, \dots, 80\%, 90\%$). Green: Random baselines; blue: real language; red: maximum-likelihood grammars fit to real orderings.}\label{tab:medians}
%\end{center}
%
%\begin{center}
%\begin{longtable}{ccccccccccccccclll}
%\input{tables/medians_REAL_1.tex}
%\end{longtable}
%	\captionof{figure}{Medians (cont.)}
%\end{center}
%
%\begin{center}
%\begin{longtable}{ccccccccccccccclll}
%\input{tables/medians_REAL_2.tex}
%\end{longtable}
%	\captionof{figure}{Medians (cont.)}
%\end{center}
%
%\begin{center}
%\begin{longtable}{ccccccccccccccclll}
%\input{tables/medians_REAL_3.tex}
%\end{longtable}
%	\captionof{figure}{Medians (cont.)}
%\end{center}
%


%
%\subsection{Surprisal at Maximum Memory}
%
%
%\begin{center}
%\begin{longtable}{ccccccccccccccclll}
%\input{tables/slice-hists_REAL_0.tex}
%\input{tables/slice-hists_REAL_1.tex}
%\input{tables/slice-hists_REAL_2.tex}
%\input{tables/slice-hists_REAL_3.tex}
%\end{longtable}
%	\captionof{figure}{Histograms: Surprisal, at maximum memory.}\label{tab:slice-hists-real}
%\end{center}
%

%\begin{table}
%\begin{tabular}{cccccccccccccccccc}
%\input{tables/quantiles_REAL_0.tex}
%\end{tabular}
%	\captionof{figure}{Quantiles: At a given memory budget, what percentage of the baselines results in higher listener surprisal than the real language? Solid curves represent sample means, dashed lines represent 95 \% confidence bounds; dotted lines represent 99.9 \% confidence bounds. At five evenly spaced memory levels, we provide a p-value for the null hypothesis that the actual population mean is $0.5$ or less. Confidence bounds and p-values are obtained using an exact nonparametric method (see text).}\label{tab:quantiles}
%\end{center}
%
%\begin{table}
%\begin{tabular}{cccccccccccccccccc}
%\input{tables/quantiles_REAL_1.tex}
%\end{tabular}
%	\captionof{figure}{Quantiles (part 2)}
%\end{center}
%
%\begin{table}
%\begin{tabular}{cccccccccccccccccc}
%\input{tables/quantiles_REAL_2.tex}
%\end{tabular}
%	\captionof{figure}{Quantiles (part 3)}
%\end{center}
%
%
%












%
%
%\subsection{Samples Drawn (Experiment 3)}
%
%
%
%\begin{center}
%\begin{tabular}{l|ll||l|llllllllllllll}
%	Language & Base. & MLE & Language & Base. & MLE \\ \hline
%\input{tables/samplesNumber_ground.tex}
%\end{tabular}
%	\captionof{figure}{Experiment 3: Samples drawn per language according to the precision-dependent stopping criterion.}\label{tab:samples}
%\end{center}
%
%
%
%\subsection{Medians (Experiment 3)}
%
%\begin{center}
%\begin{longtable}{ccccccccccccccclll}
%\input{tables/medians_0.tex}
%\input{tables/medians_1.tex}
%\input{tables/medians_2.tex}
%\input{tables/medians_3.tex}
%\end{longtable}
%	\captionof{figure}{Experiment 3. Medians: For each memory budget, we provide the median surprisal for real and random languages. Solid lines indicate sample medians, dashed lines indicate 95 $\%$ confidence intervals for the population median. Green: Random baselines; blue: real language; red: maximum-likelihood grammars fit to real orderings.}\label{tab:medians}
%\end{center}
%
%
%
%
%
%\begin{center}
%\begin{longtable}{ccccccccccccccccll}
%\input{tables/medianDiff_0.tex}
%\input{tables/medianDiff_1.tex}
%\input{tables/medianDiff_2.tex}
%\input{tables/medianDiff_3.tex}
%\end{longtable}
%	\captionof{figure}{Median Differences between Real and Baseline: For each memory budget, we provide the difference in median surprisal between real languages and random baselines; for real orders (blue) and maximum likelihood grammars (red). Lower values indicate lower surprisal compared to baselines. Solid lines indicate sample means. Dashed lines indicate 95 $\%$ confidence intervals.}\label{tab:median_diffs}
%\end{center}
%
%
%
%
%
%
%
%\begin{center}
%\begin{longtable}{cccccccccccccccccc}
%\input{tables/quantiles_noAssumption_0.tex}
%\input{tables/quantiles_noAssumption_1.tex}
%\input{tables/quantiles_noAssumption_2.tex}
%\end{longtable}
%	\captionof{figure}{Quantiles: At a given memory budget, what percentage of the baselines results in higher listener surprisal than the real language? Solid curves represent sample means, dashed lines represent 95 \% confidence bounds; dotted lines represent 99.9 \% confidence bounds. At five evenly spaced memory levels, we provide a p-value for the null hypothesis that the actual population mean is $0.5$ or less. Confidence bounds and p-values are obtained using an exact nonparametric method (see text).}\label{tab:quantiles}
%\end{center}
%
%



\section{Details for Neural Network Models}


\section{N-Gram Models}


\input{control-ngrams.tex}



%-- English, Korean, Russian
%-- UD$\_$Polish-LFG (released in 2.2, not included in original experiment) (13,744 sentences)
%-- character-level Russian
%\section{Character-Level Modeling}
%\section{Non-UD Dependency Treebanks}
%- other treebanks
%-- spoken Japanese (T{\"u}ba-J/S)
%-- another Vietnamese dependency treebank \citep{nguyen-bktreebank:-2017} (5,639 sentences)
%-- another Chinese dependency treebank LDC2012T05
%Due to the sizes of these treebanks, can also do experiment with full word forms.
%
%
%\section{Constituency Treebank}
%
%-- Penn treebank \citep{marcus-building-1993}
%
%-- spoken English (T{\"u}ba-E/S)
%
%-- spoken German (T{\"u}ba-D/S)
%
%-- Chinese treebank \citep{xue-chinese-2013}

\section{Chart Parsing Control}

LSTMs and n-gram models are linear sequence models that might incorporate biases towards linear order as opposed to hierarchical structure.
Here we use chart parsing to show that the results also hold when estimating $I_t$ using a model that is based on hierarchical structure and incorporates no bias towards linear closeness.

\subsection{Deriving PCFGs from Dependency Corpora}



We binarize, and give assign nonterminal labels to intermediate nodes based on (1) the POS of the head, (2) the lexical identity of the head, (3) the dependency label linking head and dependent.
We binarize so that left children branch off before right children.

The preterminals are labeled by POS tag and lexical identity.

It is necessary to reduce the number of preterminals and nonterminals, both to deal with data sparsity, and to make chart parsing tractable.
In our implementation for calculating $I_t$ (see below), we found that up to 700 nonterminals were compatible with efficient inference.
(For comparison, the Berkeley parser uses X nonterminals for its English grammar, but employs a highly optimized coarse-to-fine strategy.)

We reduced the number of nonterminals as follows:
(1) For words with frequency below a threshold parameter, we did not record lexical identity in preterminals and nonterminals.
(2) Nonterminals that only differ in the relation label were merged if their frequency fell below a threshold parameter,
(2) Nonterminals that only differ in the head's lexical identity were merged if their frequency fell below a threshold parameter.

Words occurring less than 3 times in the dataset were replaced by OOV.

Alternative: merge-and-split, but that would have taken too long to run on all the corpora.

We chose the threshold parameters for (1)-(3) separately for each language by sampling 15 configurations, and choosing the one that minimized estimated surprisal (see below) on a sampled baseline grammar, while resulting in at most 700 nonterminals and preterminals.

mention approaches in the literature

An alternative avoiding binarization would be to use the Earley parser, but that would have made it less feasible to parallelize processing on GPUs (see below).

\subsection{Estimating $I_t$ with Chart Parsing}
algorithm, cite Goodman's thesis

Calculating $I_t$ requires estimating entropies $H[X_1, \dots, X_t]$, and thus probabilities $P(X_1, \dots, X_t)$.
This is challenging because it requires marginalization over possible positions in a sequence.

There is a known extension of the CKY algorithm that calculates \emph{prefix} probabilities
\begin{equation}
P[\#, X_1, \dots, X_t] := \sum_N \sum_{Y_{1\dots N}} P(\#, X_1, \dots, X_t, Y_{1\dots N}, \#)
\end{equation}
(here, $\#$ denotes the beginning/end of a sentence), that is, the probability mass assigned to all sentences starting with the given prefix $X_1, \dots, X_t$.

However, simultaneously summing over possible left \emph{and} right continuations is more challenging.
(CITE) describe a method for calculating infix probabilities, but that method computes something subtly different from the quantity required here, and it is also computationally costly.
We approach this by restricting the summation on the left to prefixes of a fixed length:
\begin{equation}
    \sum_{Y_1\dots Y_N} P(\#, Y_1 \dots Y_N, X_1, \dots, X_t)
\end{equation}
and estimating
\begin{equation}
    P(X_t|X_1\dots X_{t-1}) \approx \E_{Y_1\dots Y_N} P(X_t|\#, Y_1 \dots Y_N, X_1, \dots, X_{t-1})
\end{equation}
Under certain conditions on the PCFG, this approximation converges to the tru value for sufficiently large values of $N$.\footnote{TODO say something about Markov chain convergence: For each $t$, consider for each nonterminal $\tau$ the number $n_\tau$ of nodes with this nonterminal dominating $w_\tau$. This is a Markov chain.}
Empirically, we found that the values already became essentially stationary at $N\geq 5$.

The resulting algorithm is shown in X.

For computational efficiency, we estimated $I_t$ for $t=1, \dots 5$, finding $I_t$ to be very close to zero for higher $t$.

We ran the algorithm on all contiguous sequences of length $T=5$.
Following (CITE), we took advantage of GPU parallelization for implementation, processing 1,000 sequences in parallel.


\subsection{Results}
We computed $I_t$ for the MLE grammar and for five random baseline grammars.

We did not run this on the observed orderings, as these may have crossing branches, making binarization difficult and thus rendering comparison with baselines less straightforward.

Figure~\ref{fig:resu-pcfg}

Limitations: Absolute numbers aren't comparable with other models because there are many OOVs (they are necessary because the number of non- and preterminals has to be kept low).
Also, the amount of exploited predictive information $\sum_t I_t$ is much lower than in the other models. Agrees with the observation that PCFG independence assumptions are inadequate, and that chart parsers have not historically reached good perplexities (parsers with good perplexities such as Roark Parser and RNNGs do not make these independence assumptions, but also do not allow efficient exact chart parsing).
Nonetheless, the experiment confirms the finding with a model that is based on hierarchical syntactic structure while enabling exact inference.

\begin{center}
\includegraphics[width=\textwidth]{results-table-pcfg.pdf}
\captionof{figure}{PCFG estimator, comparing fitted grammars (blue) with baselines (red)}\label{fig:resu-pcfg}
\end{center}




\section{Morphology}

Bybee 1985:

verb-valence-voice-aspect-tense-mood-modality-subj.person-subj.number

can also directly quantify MI with verb stem (presence \& choice for each slot) as a sanity check


\subsection{Japanese}


\paragraph{Data Selection}

UD corpora 2.4 with freely available word forms. Only training set, so only GSD \cite{tanaka2016universal, asahara2018universal}.

Verb forms are tokenized so that most suffixes are segmented as individual words.

We selected all chains VERB AUX...AUX. If a -te followed, added this.

We obtained 15281 forms, with a median number of X suffixes (max=...).

Describe format: form vs lemma.


Japanese verb stems and suffixes show alternations conditioned on neighboring suffixes.
In the GSD treebank, each stem and suffix is annotated with a `lemma' indicating a normalized context-independent representation of the morpheme.

In the UD treebanks, each morpheme is 


\paragraph{Verb Suffixes in Japanese}

Japanese orthography does not indicate word boundaries, and there is no universally accepted segmentation.
We determined a set of frequent morphemes as follows.
We selected all morphemes occurring at least 50 times and annotated their meaning/function.
Also, we excluded three morphemes that are treated as independent words, not suffixes, by \cite{kaiser2013japanese} (dekiru, naru, yoo).
Also, passive and potential markers are formally identical for many verbs; we included both here.

Also, we took suru to be a unit with the preceding element (it is used for turning Sino-Japanese words into verbs).

We list the morphemes according to the order extracted according to the model.
While this ordering matches almost all observed forms, there are some orderings that do not fit into this pattern, see below.



TODO read Kaiser p. 398 potential

\begin{enumerate}
\item VALENCE: causative (-ase-). \cite[142]{hasegawa2014japanese} \cite[Chapter 13]{kaiser2013japanese} (saseru, seru. 190)
\item VOICE: passive (-areru, -rareru). passive \cite[152]{hasegawa2014japanese} \cite[Chapter 12]{kaiser2013japanese} -areru, -rareru. (rareru, reru. 2000)
\item POTENTIAL: (e.g., -eru in UD)  atsuka-e-nai `can't handle' %扱えない GSD treebank
araso-e-nai `can't dispute' % 争えない GSD treebank

how is this ordered relative to PASSIVE? In UD, they don't seem to be always separated (ambiguity of reru/rareru. \cite[346]{vaccari1938complete}: identical to the passive for one class, or -eru for other verb class).
\item POLITENESS (masu, mashi, mase. 600) \cite[190]{kaiser2013japanese}. % 見できます, 見ましたい

It is interesting that this is relatively close to the verb

\item MODALITY: -ta- (-tai, -taku-, -taka-) desiderative (tai. 85) \cite[238]{kaiser2013japanese}. mi-mashi-tai `I want to see'
\item NEGATION: negation (-nai, -n-, -nakaC-. 630). also -mai for negative+volition.
\item TENSE/ASPECT/MOOD:

-ta for past (4000)

-yoo for hortative, future, ... 92 \cite[229]{kaiser2013japanese}. Q how do-nai- and -yoo interact?
\item -te derives a nonfinite form \cite[186]{kaiser2013japanese}. 4000
\end{enumerate}

Parallel to Bybee's hierarchy: valence closest, then voice.

Difference from Bybee' hierarchy: mood/modality are ordered closer than tense/aspect.

EXAMPLES:

\begin{tabular}{lllllllll|lllllll}
     & VALENCE   & VOICE    & \multicolumn{3}{c}{MOOD/MODALITY} & NEGATION & TENSE \\
Stem & Causative & Passive & Potential & Politeness & Desiderative & Negation & TAM & -te & \\ \hline
mi &          &      & &            &          & naka     & tta &    & did not see \cite[153]{vaccari1938complete}\\
mi &          &      & &            & taku     & nai      &     &     & I do not wish to see \cite[98]{vaccari1938complete} \\
mi &          &      & &            & taku     & naka    & tta  &      & I did not wish to see \cite[98]{vaccari1938complete} \\
oyog & ase    &      & &            &          &         & ta    &     & made swim \\
tat & ase     & rare & &            &          &         & ta    &     & was made to stand up \cite[396]{kaiser2013japanese} \\
waraw  &       & are & &            &           &         & ta    &    & was laughed at \cite[384]{kaiser2013japanese} \\
mi     &       & rare& & mase       &          & n        &      &     & is not seen \cite[337]{vaccari1938complete} \\
mi     &       & rare& & mash      &           &          & yoo  &    & will be seen \cite[337]{vaccari1938complete} \\
de     &       &    & &            &           & naka     & roo  &    & will not go out \cite[170]{vaccari1938complete} \\
mi     &       &    & e & mase       &           & n        &      &    & cannot see \cite[349]{vaccari1938complete} \\
mi     &       &    & & mashi      & tai       &          &      &    & want to see
\end{tabular}


Not all combinations are possible. 
`was not seen' is miraremasen deshita \cite[337]{vaccari1938complete}, not MIRU-RERU-MASE-NAI-TA
Negation is not directly combined with -yoo.

Some relatively frequent morphemes that are below the threshold

\begin{enumerate}
\item nakereba `if not' (cf \cite[9.3.1.4]{kaiser2013japanese})
\item tsudzukeru (continuative, 30)
\item yasui `easy to'
\item hajimeru (inchoative, 21)
\item naru (`become', 102). Maybe this should be excluded, not treated as an affix in the books.
\item yooda (180) EVIDENTIAL? yooda. but past -ta- can be after it: iruyoodatta IRU-YOODA-TA (Kaiser et al 9.5.6.1.1.1) vs misenakattayooda MISERU-NAI-TA-YOODA `it didn't seem' (GSD treebank). Maybe this should be thought of better as an SCONJ at least when including -ni, Vaccari p. 238-241.
\item beki Necessitative \cite[248]{kaiser2013japanese}
\item soo `likely to' \cite[258]{kaiser2013japanese}, probably not AUX in UD
\item dekiru potential (180), 
\end{enumerate}


there's just too many verb forms, here limited to frequent ones we found in the corpus.


\paragraph{Experiment}

modes:

- predicting on the phoneme level

- predicting on the grapheme level: equivalent to phonemic, but in units of syllables

- predicting on the level of morphemes: we take the lemmas, and unite saseru+seru to CAUSATIVE and reru+rareru+eru+keru to PASSIVE/POTENTIAL.

This controls for allomorphy / morphophonemic changes, which are conditioned on surrounding affixes (so naturally favor locality).


comparison to baselines

- random grammars

optimization:

- phonemes

- graphemes

- morphemes



% volitional 加わろう kawawar-oo
% volitional polite 加わりましょう kawawar-i-mashy-oo
% volitional neg 加わるまい(+), 加わらない[よう/こと]にしよう kawawaru-mai, kawawara-nai-yoo-nishyoo
% volitional neg polite 加わりますまい, 加わらない[よう/こと]にしましょう  kawawari-masu-mai, kawawar-anai-yoo-nishimashyoo

%  verb-valence-voice-aspect-tense-mood-modality-person-number

% http://nihongo.monash.edu/cgi-bin/wwwjdic?1W%B2%F1%B5%C4%A4%CB%B2%C3%A4%EF%A4%EB_v5r
% passive negative
% 加わられない kuwawar-are-nai

% passive negative polite
% 加わられません kuwawar-are-mase-n

% causative polite
% 加わらせます kuwawar-ase-masu

% causative negative polite
% 加わらせません kuwawar-ase-mase-n
% 加わらしません kuwawar-ashi-mase-n

% da/de/na (lemmatized as da): copula (?). na after adjectives.




honorific -masu

-s-are-naka-tta ?passive+negative.past?



% -u/-ru (non-past) -- not segmented off in UD?

% suru - reru - ta
% suru -masu - ta
% reru - ta
% suru - ta
% masu - ta
% rareru - ta
% masu - nai - deshita
% suru - reru
% suru - nai
% masu - nai
% reru - masu - ta
% dekiru - masu

% TODO in excluding -te-, have to deal with -de- as in 読んで yonde (Vaccari p. 109)

%(('し/動詞/し', 'た/助動詞/た'), 14389)
%(('る/語尾/る',), 13880)
%(('あ/動詞/あ', 'る/語尾/る'), 11481)
%(('っ/語尾/っ', 'た/助動詞/た'), 9378)
%(('さ/動詞/さ', 'れ/助動詞/れ', 'た/助動詞/た'), 7143)
%(('さ/動詞/さ', 'れ/助動詞/れ', 'る/語尾/る'), 1966)
%(('られ/助動詞/られ', 'る/語尾/る'), 1156)
%(('だっ/助動詞/だっ', 'た/助動詞/た'), 1151)
%(('られ/助動詞/られ', 'た/助動詞/た'), 1114)
%(('ん/語尾/ん', 'だ/助動詞/だ'), 854)
%((), 839)
%(('い/語尾/い', 'た/助動詞/た'), 761)
%(('わ/語尾/わ', 'れ/助動詞/れ', 'た/助動詞/た'), 715)
%(('で/助動詞/で', 'あ/動詞/あ', 'る/語尾/る'), 688)
%(('さ/語尾/さ', 'れ/助動詞/れ', 'た/助動詞/た'), 599)
%(('わ/語尾/わ', 'れ/助動詞/れ', 'る/語尾/る'), 595)
%(('れ/助動詞/れ', 'る/語尾/る'), 563)
%(('く/語尾/く',), 485)


% ('できる', 'ます'): 11, ('かもしれる', 'ます', 'ない'): 1, 
%('ない', 'た'): 29, ('ちゃう', 'ます', 'た'): 2, 
%('する', 'ます'): 23, ('られる', 'ます'): 4, ('できる',): 18, ('する', 'せる', 'た'): 12, ('たい', 'ない'): 1, ('する', 'ます', 'ない'): 4, ('せる', 'ない'): 1, ('らしい',): 2, ('出来る', 'ます', 'ない'): 1, ('かもしれる', 'ない'): 1, ('れる', 'ない', 'た'): 1, ('ない', 'だめ', 'だ'): 1, ('する', 'れる', 'そうだ'): 2, ('える',): 5, ('かねる', 'ない'): 1, ('れる', 'ようだ', 'なる', 'た'): 3, ('せる', 'ます', 'ない'): 2, ('続ける', 'た'): 1, ('られる', 'ようだ', 'なる', 'ます', 'た'): 1, ('できる', 'ます', 'た'): 7, ('する', 'せる'): 2, ('やすい', 'なる', 'た'): 1, ('くださる', 'ます'): 1, ('ようだ', 'なる', 'ます', 'た'): 2, ('える', 'ようだ', 'なる', 'た'): 1, ('する', 'がたい'): 1, ('する', 'れる', 'ます', 'た'): 6, ('ざるを得る', 'ます', 'ない'): 1, ('ようだ', 'なる', 'た'): 10, ('する', 'れる', 'ます'): 3, ('られる', 'ます', 'た'): 4, ('える', 'た'): 1, ('する', 'ます', 'う'): 1, ('する', 'ます', 'ない', 'でした'): 1, ('済み',): 1, ('ようだ',): 2, ('出す', 'た'): 1, ('出す',): 1, ('する', 'た', 'みたいだ'): 1, ('させる', 'た'): 2, ('きる', 'た'): 1, ('する', 'たい'): 2, ('続ける',): 2, ('ざるをえる', 'ます', 'ない'): 1, ('める', 'ます'): 6, ('せる', 'ます'): 1, ('ようだ', 'なる'): 7, ('られる', 'ようだ', 'なる', 'た'): 1, ('始める', 'た'): 2, ('ない', 'なる'): 2, ('かねる', 'ます'): 1, ('始める',): 2, ('する', 'れる', 'ない', 'た'): 1, ('える', 'ます'): 3, ('た', 'そうだ'): 4, ('れる', 'ます', 'ない'): 1, ('する', 'ようだ', 'なる'): 4, ('する', 'える', 'ない'): 1, ('ける', 'ます'): 3, ('できる', 'ない', 'た'): 1, ('する', 'ない', 'た'): 2, ('出来る', 'ます'): 1, ('れる', '始める'): 1, ('ます', 'う'): 4, ('する', '始める', 'た', 'そうだ'): 1, ('する', 'れる', 'た', 'ようだ'): 1, ('できる', 'まい'): 1, ('た', 'らしい'): 1, ('する', 'ようだ', 'なる', 'た'): 2, (' える', 'ます', 'ない'): 1, ('ける',): 2, ('られる', 'た', 'そうだ'): 1, ('られる', 'そうだ'): 1, ('れる', 'にくい'): 1, ('する', 'やすい'): 1, ('ける', 'た'): 1, ('する', 'れる', 'ます', 'ない'): 2, ('そ うだ',): 1, ('ない', 'た', 'ようだ'): 1, ('ます', 'たー'): 1, ('できる', 'た'): 3, ('べる',): 1, ('する', 'れる', 'ようだ', 'なる', 'ます', 'た'): 1, ('ない', 'ようだ'): 1, ('ける', 'ます', 'う'): 1, ('ちゃう', 'ます'): 1, ('する', 'そうだ'): 1, ('たい', 'なる'): 1, ('ようだ', 'なる', 'ます'): 1, ('たい', 'ない', 'た'): 1, ('やすい',): 3, ('かける',): 1, ('ない', 'なる', 'ようだ', 'なる', 'た'): 1, ('ない', 'そうだ'): 1, ('する', 'たい', 'ない', 'らしい'): 1, ('する', 'ちゃう', 'ます'): 1, ('た', 'ようだ'): 1, ('られる', 'たい', 'ない'): 1, ('える', 'ない'): 1, ('てる', 'ない'): 1, ('する', '易い', 'なる', 'た'): 1, ('れる', 'ない'): 1})


% nai (negation)





\subsection{Sesotho}

\cite{doke1967textbook}

We use the Demuth Corpus \cite{demuth1992acquisition} of child and child-directed speech, containing about 13K utterances with 500K morphemes.


The corpus has very extensive manual morphological segmentation and annotation; each verb form is segmented into morphemes, which are annotated for their function.

Sometimes morphemes fused, indicated with slash in the annotation.

Sesotho has composite forms consisting of an inflected auxiliary followed by an inflected verb.
Both verbs carry subject agreement.
While they are annotated as a unit in the Demuth corpus, they are treated as separate words in grammars \citep{doke1967textbook,guma1971outline,lombard1969handbook}.
We separated these, taking the main verb to start at its subject agreement prefix.
We only considered main verbs for the experiments here.

Forms in child utterances are annotated with well-formed adult forms; we took these here.


Obtained 37K verbs.

% TODO analysis for adult-oly portion

\paragraph{Verb Prefixes in Sesotho}


According to \cite{demuth1992acquisition}:
 
 SM-(T/A)-(OBJ)-V-(EXT)/(PERF)/(PASS)-M 

Can add negation, interrogative/relative markers, and some ordering relations.
 

In the Demuth corpus, each morpheme is annotated; a two-letter key indicates the type of morpheme (e.g. subject agreement, TAM marker).
We classified morphemes by this annotation.
We considered morpheme types occurring at least 50 times in the corpus.

As in Japanese, morphemes show different forms depending on their environment, and the corpus contains some instances of fused neighboring morphemes that were not segmented further.

\begin{enumerate}


    \item SUBJECT agreement
    
    %(sm- 17K or sr- 193): person/number or noun class. also has variation depending on verb form (e.g. 1st sg is ke-, ka-, N- depending on form, Guma 1971, p. 162). See Lombard 1985, p. 104-106 for morphophonological rules.
    
    \cite[\textsection 395]{doke1967textbook} for sm-
    
    \item NEGATION Negation 
    
    %(ska, seka, sa, skaba, 362). sa used in Negative Dependent Present (Paroz 1946, p.31) and Perfect (ibid p. 80). E.g. Sec 13.38 p.172 in Guma 1971. -sa- and -se- Lombard 1985 p. 108, appear in different moods.
    %What is the order of NEGATION and TENSE/ASPECT?
    
    \item TENSE/ASPECT/MOOD Tense/aspect marker (t\^{} 13K) Guma 1971, p. 165-166. -a- Lombard 1985, p. 109.
    
    -tla-, -tlo-, -ilo- FUT \cite[\textsection 410]{doke1967textbook}
    
    -a- PRS \cite[\textsection 400]{doke1967textbook}
    
    -ka- POT \cite[\textsection 424]{doke1967textbook}
    
    -sa- PERSIST \cite[\textsection 414]{doke1967textbook}
    
    -tswa- RECENT.PST \cite[\textsection 405]{doke1967textbook} 
    
    ..
    Past subjunctive
    
    
    Potential -ka- (cannot be combined with negation marker in the previous slot, \cite[\textsection 424]{doke1967textbook}).
    
    
    Some of the common markers in the corpus are t\^{}f for future and t\^{}pf for perfect.
    
    (e.g., ile- for remote past \cite[\textsection 177]{doke1967textbook})
    
    Also often fused with the object agreement marker.
    
    
    \item OBJECT Object agreement (om 6K) or reflexive (rf, 751). These are mutually exclusive \cite[p. 165]{guma1971handbook}
\end{enumerate}



Additionally FINITENESS (?) Conditional morpheme (\emph{ho}, `if', 314), comes before object agreement.

Parallel to Bybee hierarchy: subject agreement is farther away from the verb than TAM.


TODO ha- negation is a prefix coming before the subject agreement prefix (Guma 1971, p. 164).  ga- in Lomard 1985 p. 108.



Also some merging between tense and object markers. for the lemma-based version, have to construct a version where all of this merging is undone (e.g. including a slash, or t\^{}.om2s)


\paragraph{Verb Suffixes in Sesotho}

Again, we extracted morpheme types occurring at least 50 times.

\begin{enumerate}
    \item VALENCE:
    
    
    
    - causative (c 1K), -isa (with morphophonological change) \cite[\textsection 325]{doke1967textbook}
    
    See \cite[\textsection 345]{doke1967textbook} for reversive+causative.
    
    - neuter/stative (nt, 229), -eha, -ahala \cite[\textsection 307]{doke1967textbook}
    
    
    - reversive (rv, 214),  \cite[\textsection 345]{doke1967textbook} -- not really VALENCE, ex. bind -- loosen
    
    General references (comparative Bantu):

Schadeberg, Thilo C. 2003. Derivations. In Derek Nurse and Gerard Philippson (eds.), The
Bantu languages, 71–89. London: Routledge.

Doke, Clement. 1935. Bantu linguistic terminology. London: Longman, Green \& Co

Guthrie, Malcolm. 1962. The status of radical extensions in Bantu languages. Journal of
African Languages 1(3). 202–220.

    
    
    - applicative
    (ap -el- \cite[\textsection 310]{doke1967textbook} \cite[p. 109]{lombard1969handbook}, with morphophonological changes), 2K times
    
    Example from Demuth corpus: 
    
    I am lighting for you
    
    ke-a-u-khan-ts-ets-a
    
    sm1s-t\^{}p-om2s-shine-c-ap-m\^{}in
    
    
    Leave my blanket alone then
    
    tl-oh-el-a kobo ya-ka he
    
    come-rv-ap-m\^{}i blanket(9,10) 9-1s ij
    
    
    \cite[\textsection 314-315]{doke1967textbook}: applicative can be applied to other valence affixes
    
    -  Perfective/Completive (-ets), -ell (66 times) \cite[\textsection 336]{doke1967textbook}
    
    This isn't really VALENCE. It is a reduplication of the applicative suffix \cite[\textsection 336]{doke1967textbook}, and as such is ordered before passive (u-neh-el-ets-w-a-ng t\^{}p.om2s-give-ap-cl-p-m\^{}in-wh What is it that you want passed to you?)
    
    - rc reciprocal (-an 103 times)  \cite[\textsection 338]{doke1967textbook}
    
    Comes after applicative: 
    
    Do they share?
    
    ba-arol-el-an-a
    
    sm2-t\^{}p\_divide-ap-rc-m\^{}in
    
    \item VOICE: passive -iw- (1K)
    
    \cite[\textsection 300]{doke1967textbook} \cite[p. 114]{lombard1969handbook}
    
    Passive follows applicative: sho-el-oa die-APPL-PASS \cite[\textsection 324]{doke1967textbook}
    
    
    \item TENSE: tense (t\^{}, 3K) .
    
    
    The perfect affix has the form -il-, which has a range of allomorphs depending on the preceding stem and valence/voice suffixes, if present \cite[\textsection 369]{doke1967textbook}, \cite[p. 167]{guma1971outline}, \cite[p. 116]{lombard1969handbook}.
    Common morphs in the Demuth corpus are -il- and -its-.
    
     
    \item MOOD: Mood (m\^{}, 37K times)
    
    
    
    Demuth corpus:
    
    - IMP (-e)
    
    - SBJV1 (-e)
    
    - SBJV1.PL (-eng)
    
    - IND (-a, -e)
    
    - IMP.PL (-ang)
    
    - SBJV2 (-e, -a)
    
    - NEG  (-e, -a)
    
    % Demuth:
    %The mood of infinitives is m^in if affirmative indicative, m^x if neg in indicative assertive, declarative, affirmative statements 
    %pt participial in compound tenses, subordinate clauses, relative clauses 
    %x negative in negative utterances ha ke-tseb-e ng sm1s-t^p_v^know-m^x I don’t know 
    %i imperative m-ph-e om1s-v^give-m^i Give me. 
    %ip imp plural bon-ang v^see-m^ip Look! 
    %s subjunctive ere ke-bon-e  ht sm1s-t^p_v^see-m^s Let me see. 
    %sp plural subjunctive ha re-y-eng ht sm1p-t^p_v^go-m^sp Let’s go. 
    
    \item Interrogative (2K times) and relative (857 times) markers -ng
    
    interrogative: \cite[p. 168]{guma1971handbook}, \cite[\textsection 160, 320]{doke1967textbook}
    
    relative \cite[\textsection 271, 793]{doke1967textbook}
    

\end{enumerate}

relation to Bybee hierarchy: valence closest, then voice, then tense, then mood.

Example

\begin{tabular}{lllllllll|lllllll}
SUBJECT & NEG. & TAM & OBJECT & Stem & VALENCE & VOICE & TENSE & MOOD \\
o       &          &     &        & pheh &         &       & il    & e  & (Thabo) cooked (food) (Demuth (15)) \\
ke      &          &     & e      & f   &          & uw    &      & e   & (I) was given (the book) (Demuth (26c)) \\
o       &          &     &        & pheh & el      &      &       & a & (Thabo) cooks (food for Mpho) (Demuth (41))\\
o       &          &     &        & pheh & el      & w    &      & a & (Mpho) is being cooked (food) (Demuth (42))
\end{tabular}



modes:

- phonemic

- normalized to morphemes as annotated in the Demuth corpus

- normalized to morphemes, undoing merging (as indicated in the Demuth corpus)

experiments:

- tradeoff comparison

- optimization

%('c', 'ap', 'm^'): 36, 
%('c', 'nt', 'm^'): 19, ('p', 't^', 'rl'): 12, 
%('p/', 'm^'): 1, ('wh', 'm^'): 1, ('rc', 'm^', 'rl'): 1, ('rv', 'c', 'm^'): 2, 
%('c', 'p', 'm^'): 32, ('c', 't^', 'p', 'm^'): 5, ('rc', 't^'): 17, ('c', 'ap', 'm^', 'wh'): 8, ('nt', 'm^', 'rl'): 10, ('rv', 'p', 'm^'): 3, ('ap', 't^'): 6, ('ap', 'cl', 'p', 'm^', 'wh'): 2, 
%('ap', 'm^', 'rl'): 39, ('ap', 'cl', 'm^'): 29, ('rv', 'ap', 'm^'): 4, ('c', 'm^', 'wh'): 11, ('ap', 'ap', 'p', 'm^', 'rl'): 1, ('rv', 't^', 'm^', 'rl'): 2, ('c', 't^'): 5, ('c/', 'p', 'm^'): 6, ('t^', 'rl'): 6, ('c', 'rc', 'm^'): 4, ('m.',): 1, ('rc', 'c', 'rc', 'm^'): 1, ('wh',): 1, ('nt', 't^'): 1, 

%('p', 'm^', 'rl'): 38, 

%('p', 't^', 'm^'): 1, ('t^', 'wh'): 2, ('rc', 'c', 'm^'): 3, ('ap', 'c', 'm^'): 3, ('c', 'ap', 't^', 'm^'): 3, ('cl', 'm^'): 23, ('rc', 'c', 'ap', 'm^'): 1, ('c', 'p', 't^'): 1, ('ap', 't^', 'm^', 'wh'): 2, ('p', 't^', 'wh'): 1, ('ap', 't^', 'm^'): 20, ('nt', 't^', 'm^', 'rl'): 2, ('ap', 't^', 'm^', 'rl'): 3, ('c', 'p', 'm^', 'rl'): 1, ('m^', 'lc'): 1, ('rv', 'p', 'm^', 'rl'): 1, ('ap', 'm^', 'lc'): 1, ('c', 'cl', 'm^'): 1, ('ap', 't^', 'p', 'm^'): 3, ('ap', 'ap', 'm^', 'wh'): 2, ('cp', 't^', 'm^'): 1, ('ap', 'ap', 't^', 'm^'): 3, ('c/', 'm^', 'wh'): 1, ('ap', 't^', 'rl'): 2, ('c', 'mi'): 1, ('rv', 'nt', 't^', 'm^'): 1, ('ps', 't^'): 3, ('t^', 'p', 'm^', 'wh'): 1, ('rc', 'm^', 'wh'): 1, ('ap', 'rc', 'm^'): 5, ('rv', 'nt', 'm^'): 2, ('ap', 'cl', 't^', 'm^'): 4, ('c', 't^', 'm^', 'rl'): 8, ('rc', 'p', 'm^'): 2, ('ap', 'p', 'rc', 'wh'): 1, ('ap', 'p', 'm^', 'wh'): 2, ('ap', 'cl', 'm^', 'wh'): 1, ('c', 't^', 'rl'): 1, ('cl', 'p', 'm^', 'wh'): 1, ('rv', 't^', 'p', 'm^', 'rl'): 1, ('ap', 'c/', 'm^'): 1, ('nt', 'ap', 'm^'): 1, ('rv', 't^', 'p', 'm^'): 1, ('cl', 't^', 'm^'): 3, ('rv', 'ap', 'm^', 'wh'): 1, ('c/', 'm^', 'rl'): 1, ('c', 'ap', 'p', 'm^'): 1, ('cl', 'ap', 'm^'): 1, ('ap', 'p', 'm^', 'rl'): 1, ('cl', 'p', 'm^'): 1})







\section{Some theoretical thoughts (scratch area)}
\subsection{Learnability bounds}

\paragraph{Upper bounds via n-gram models:}
Want to show that, if we know a process has stronger locality, there is a learning algorithm with lower sample complexity.

\begin{thm}
The class of processes with fixed bounds on $H_0$ on $EE$ can be learned up to KL loss $2\epsilon$ with ... samples with prob ...
\end{thm}

We want to learn a process to average KL distance $2\epsilon$.
Assume excess entropy is $\leq I$, then only need to learn $N := I/\epsilon$-gram-model for KL loss $\epsilon$.

(Or, to get a better bound, take $N$ so that $\sum_{t=N}^\infty I_t < \epsilon$.).

So $N = \sum_{t=1}^\infty t I_t/\epsilon$.
How many samples are needed to learn this up to $\epsilon$? Probably that will scale with the block entropy, which is $N H_0 + \sum_{t=1}^N t I_t$, with $H_0$ the unigram entropy.
So the sample complexity would seem to scale with
$$\exp\left(H_0 \sum_{t=1}^\infty t I_t/\epsilon + \sum_{t=1}^{\sum_{t=1}^\infty t I_t/\epsilon} t I_t\right)$$


Want to learn $P(Y|X)$ up to $\mathbb{E}_x D_{KL}(P(y|x)||\hat P(y|x)) \leq \epsilon$.
How many i.i.d. samples from $(X,Y)$ do we need?

Assume the distribution of $Z$ has `low' entropy.
How many i.i.d. samples do we need to get a good estimate of $P(Z)$?
Something scaling with $\exp(H(Z))$?

Convergence

references:

\url{https://arxiv.org/pdf/1904.02291.pdf} leads to:

$$Pr(D_{KL}(\hat P(z)||P(z)) \geq \epsilon) \leq e^{-n\epsilon} \left(\frac{e\epsilon n}{|V|-1}\right)^{|V|-1}$$
if $V$ is the set of values of $Z$, when $\epsilon > \frac{|V|-1}{n}$.


Is it possible to get something similar but with $|V|$ replaced with $H[X]+|V_Y|$?

\begin{thm}(must be something standard)
Let $X$ be an RV. Then $1-\epsilon$ of its probability mass is concentrated on at most
???
values.
\end{thm}

\begin{proof}
Assume no way of covering $1-\epsilon$ probability mass takes less than $K$ values.
That is (ordering $p_i$ by magnitude downwards):
$$\sum_{i=1}^K p_i \leq 1-\epsilon$$

Want to show that $H[X]$ cannot be too small.

First, note

$(1-\epsilon) - (K-1) \frac{1-\epsilon}{|V|-K} \geq p_1 \geq \dots \geq p_K \geq \frac{1-\epsilon}{|V|-K}$

or (reformulating slightly)

$(1-\epsilon) (1 - (K-1) \frac{1}{|V|-K}) \geq p_1 \geq \dots \geq p_K \geq \frac{1-\epsilon}{|V|-K}$

$H[X] \geq \sum_{i=1}^K \frac{1-\epsilon}{|V|-K} \log \frac{1}{(1-\epsilon) (1 -  \frac{K-1}{|V|-K})} = K \frac{1-\epsilon}{|V|-K} \log \frac{1}{(1-\epsilon) (1 -  \frac{K-1}{|V|-K})} = K \frac{1-\epsilon}{|V|-K} \log \frac{1}{(1-\epsilon) \frac{|V|-K-K+1}{|V|-K})} = K \frac{1-\epsilon}{|V|-K} \log \frac{{|V|-K}}{(1-\epsilon) (|V|-2K+1)}$


So
$H[X] \geq K \frac{1-\epsilon}{|V|-K} \log \frac{{|V|-K}}{(1-\epsilon) (|V|-K)} = \frac{K}{|V|-K} (1-\epsilon) \log \frac{1}{(1-\epsilon)}$

So

$\frac{K}{|V|-K} \leq H[X] \frac{1}{(1-\epsilon) \log \frac{1}{(1-\epsilon)}}$

However, this bound does not seem very useful, as it never allows one to conclude something like $K << |V|$.

\end{proof}


%Then
%$$H[X] \leq (1-\epsilon) \log \frac{K}{1-\epsilon} + \epsilon \log \frac{|V|-K}{\epsilon}$$
%So
%$$H[X] \leq (1-\epsilon) \log \frac{K}{1-\epsilon} + \epsilon \log \frac{|V|}{\epsilon}$$
%So
%$$H[X] - \epsilon \log \frac{|V|}{\epsilon} \leq (1-\epsilon) \log \frac{K}{1-\epsilon}$$
%So
%$$\exp\left(\frac{H[X] - \epsilon \log \frac{|V|}{\epsilon}}{1-\epsilon}\right) (1-\epsilon) \leq K$$
%Taking the contrapositive, if
%$$\exp\left(\frac{H[X] - \epsilon \log \frac{|V|}{\epsilon}}{1-\epsilon}\right) (1-\epsilon) > K$$
%then $$\sum_{i=1}^K p_i > 1-\epsilon$$


To get intuition, if $I_t = \alpha \cdot t^{-3}$, then excess entropy $I=\alpha \pi^2/6$, and (bounds are a bit crude here)
\begin{align}
& H_0 \sum_{t=1}^\infty t \alpha t^{-3}/\epsilon + \sum_{t=1}^{\sum_{t=1}^\infty t \alpha t^{-3}/\epsilon} t \alpha \cdot t^{-3} \\
= & H_0 \alpha/\epsilon \cdot \sum_{t=1}^\infty  t^{1-3} + \sum_{t=1}^{\sum_{t=1}^\infty t \alpha t^{-3}/\epsilon} t \alpha \cdot t^{-3} \\
\leq & H_0 \alpha/\epsilon \cdot \pi^2/6 + \sum_{t=1}^{\alpha/\epsilon \cdot \pi^2/6} t \alpha \cdot t^{-3} \\
= & \frac{H_0 \alpha \pi^2}{6 \epsilon} + \alpha \sum_{t=1}^{\alpha/\epsilon \cdot \pi^2/6}  t^{-2} \leq  \frac{H_0 \alpha \pi^2}{6 \epsilon} + \alpha \pi^2/6  \leq  \left(\frac{H_0}{\epsilon} + 1\right) \frac{\alpha \pi^2}{6} \\
\end{align}


Question: Can there be meaningful lower bounds in terms of locality?

Question: Can we give a lower bound by considering that learning sequences itself requires short-term memory?

try to derive something for Trading Value and Information paper


\subsection{Optimization}
Area under $t - H_t$ curve up to $T$: $\sum_{t=1}^T H_t = T H + \sum_{t=1}^T t I_t$

We cannot optimize for AUC using the method of (CITE), because we cannot construct an unbiased gradient estimator
$AUC \propto \sum_{t} (\sum_{s \leq t} I_s) tI_t = \sum_{s \leq t}  tI_t I_s = I_1^2 + 2 I_1 I_2 + 2 I_2^2 + \dots$



As a surrogate, we propose to maximize $I_1$.
This corresponds

It provides an accurate approximation to the AUC if $I_s$ is small for $s > 1$, which holds for the n-gram based estimator.

If $I_s < \epsilon$ for $s > 1$, then 
\begin{equation}
	I_1^2 \leq \sum_{1\leq s \leq t \leq T}  tI_t I_s \leq I_1^2 + I_1 \epsilon T^2 + \epsilon^2 T^2
\end{equation}
That means, $I_t^2$ is an accurate approximation of the AUC when $\epsilon T^2$ is small.


Softmax gradient update corresponds to adding $\alpha$ to the target logit and removing $\alpha$ from all other logits.
Equivalently, add a certain dynamic amount to the logcount of the target and the total logcount.



Counting corresponds to adding $1$ to the target probability. 




\bibliographystyle{apalike}
\bibliography{literature}

%\appendix




\end{document}






